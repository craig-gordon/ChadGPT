{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chagdpt\\Scripts\\activate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['chagdpt\\\\Scripts\\\\python.exe', '-m', 'ipykernel', 'install', '--user', '--name=chagdpt', '--display-name=ChagGPT'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "venv_dir = \"chadgpt_venv\"\n",
    "subprocess.run([sys.executable, \"-m\", \"venv\", venv_dir])\n",
    "\n",
    "activate_script = os.path.join(venv_dir, \"Scripts\", \"activate\") if platform.system() == \"Windows\" else os.path.join(venv_dir, \"bin\", \"activate\")\n",
    "\n",
    "venv_python = os.path.join(venv_dir, \"Scripts\", \"python.exe\" if platform.system() == \"Windows\" else \"bin/python\")\n",
    "\n",
    "subprocess.run([venv_python, \"-m\", \"pip\", \"uninstall\", \"-y\", \"torch\", \"torchvision\", \"torchaudio\"])\n",
    "\n",
    "subprocess.run([venv_python, \"-m\", \"pip\", \"install\", \"torch\", \"torchvision\", \"torchaudio\", \"--index-url\", \"https://download.pytorch.org/whl/cu118\"])\n",
    "\n",
    "subprocess.run([venv_python, \"-m\", \"pip\", \"install\", \"diffusers\", \"accelerate\", \"transformers\", \"protobuf\", \"sentencepiece\"])\n",
    "\n",
    "print(f\"Virtual environment setup complete. To activate it, run:\\nsource {activate_script}\" if platform.system() != \"Windows\" else f\"{activate_script}\")\n",
    "\n",
    "subprocess.run([venv_python, \"-m\", \"pip\", \"install\", \"ipykernel\"])\n",
    "subprocess.run([venv_python, \"-m\", \"ipykernel\", \"install\", \"--user\", \"--name=chagdpt\", \"--display-name=ChagGPT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stable Diffusion 3 Medium Diffusers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]it/s]\n",
      "Loading pipeline components...:  22%|██▏       | 2/9 [00:02<00:08,  1.21s/it]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:04<00:00,  2.24it/s]\n",
      "100%|██████████| 28/28 [00:04<00:00,  5.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# config_files = snapshot_download(\n",
    "#     \"stabilityai/stable-diffusion-3-medium\",\n",
    "#     local_dir=r\"C:\\Users\\Craig\\Desktop\\stable-diffusion-3-medium\",\n",
    "#     local_dir_use_symlinks=False,\n",
    "#     ignore_patterns=[\"*.safetensors\", \"*.bin\", \"*.ckpt\"],  # Skip large weight files\n",
    "# )\n",
    "\n",
    "# model_path = r\"C:\\Users\\Craig\\Desktop\\stable-diffusion-3-medium\"  # Remove the filename from the path\n",
    "pipeline = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\n",
    "pipeline.to(\"cuda\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "image = pipeline(\n",
    "    prompt=\"a photo of a cat holding a sign that says hello world\",\n",
    "    negative_prompt=\"\",\n",
    "    num_inference_steps=28,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=7.0,\n",
    ").images[0]\n",
    "\n",
    "image.save(\"sd3_hello_world.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pipeline.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LoRA Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=4):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.low_rank_a = nn.Parameter(torch.randn(in_features, rank))\n",
    "        self.low_rank_b = nn.Parameter(torch.randn(rank, out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + (x @ self.low_rank_a) @ self.low_rank_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject LoRA Layers Into Attention Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_lora_layers(pipeline, rank=4):\n",
    "    # Access the UNet components through the pipeline's text_encoder_2\n",
    "    for name, module in pipeline.text_encoder_2.named_modules():\n",
    "        # Look for attention blocks in the text encoder\n",
    "        if \"self_attn\" in name:\n",
    "            # Modify the query, key, and value projections\n",
    "            if hasattr(module, \"q_proj\"):\n",
    "                in_features = module.q_proj.in_features\n",
    "                out_features = module.q_proj.out_features\n",
    "                module.q_proj = nn.Sequential(\n",
    "                    module.q_proj,\n",
    "                    LoRALayer(in_features, out_features, rank=rank).to(dtype=torch.float16)\n",
    "                )\n",
    "            if hasattr(module, \"k_proj\"):\n",
    "                in_features = module.k_proj.in_features\n",
    "                out_features = module.k_proj.out_features\n",
    "                module.k_proj = nn.Sequential(\n",
    "                    module.k_proj,\n",
    "                    LoRALayer(in_features, out_features, rank=rank).to(dtype=torch.float16)\n",
    "                )\n",
    "            if hasattr(module, \"v_proj\"):\n",
    "                in_features = module.v_proj.in_features\n",
    "                out_features = module.v_proj.out_features\n",
    "                module.v_proj = nn.Sequential(\n",
    "                    module.v_proj,\n",
    "                    LoRALayer(in_features, out_features, rank=rank).to(dtype=torch.float16)\n",
    "                )\n",
    "\n",
    "inject_lora_layers(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Image Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, image_dir, prompts, image_processor, image_size=(1024, 1024)):\n",
    "        self.image_dir = image_dir\n",
    "        self.prompts = prompts\n",
    "        self.image_processor = image_processor\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, f\"{idx}.jpg\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = image.resize(self.image_size, Image.Resampling.LANCZOS)\n",
    "        prompt = self.prompts[idx]\n",
    "\n",
    "        # Process the image and convert to float16\n",
    "        processed_image = self.image_processor.preprocess(image)\n",
    "        if isinstance(processed_image, dict) and \"pixel_values\" in processed_image:\n",
    "            processed_image[\"pixel_values\"] = processed_image[\"pixel_values\"].to(dtype=torch.float16)\n",
    "\n",
    "        return {\n",
    "            \"image\": processed_image,\n",
    "            \"prompt\": prompt\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image type: <class 'torch.Tensor'>\n",
      "Prompt: Gigachad smiling confidently and facing to the left from the chest up\n"
     ]
    }
   ],
   "source": [
    "image_dir = r\"C:\\Users\\Craig\\Desktop\\stable-diffusion-3-medium\\gigachads\"\n",
    "prompts = [\n",
    "    \"Gigachad smiling confidently and facing to the left from the chest up\",\n",
    "    \"Gigachad shirtless with a muscular, veiny body facing ahead with a serious expression\",\n",
    "    \"Gigachad sitting in a chair with his arms on his legs looking to the right with a relaxed, slightly melancholic expression\",\n",
    "    \"Gigachad standing straight with his full torso pictured in only black underwear and looking straight ahead with a piercing gaze\",\n",
    "    \"Gigachad shirtless from the chest up staring straight ahead with his hair slicked back\",\n",
    "    \"Gigachad facing to the left with his gaze pointed down and to the right and with a thin rope wrapped around his upper body\",\n",
    "    \"Gigachad standing slightly leaned to the left wearing jeans and no shirt and smiling voraciously, with his hands behind his back\",\n",
    "    \"A close up of Gigachad's face and shoulders, with his right arm behind his head and Gigachad facing to the right with an intense expression\",\n",
    "    \"Gigachad wearing jeans with his left arm extended rightward and his right hand clasping his left bicep\",\n",
    "    \"A close up of Gigachad's upper body, with his right arm reaching behind his head causing a shadow to cover the right side of his face\",\n",
    "    \"Gigachad with a smug expression, leaning slightly to the left, wearing small black underwear and a tattoo on his lower abdominal muscles that says Berlin 1969\",\n",
    "    \"Gigachad balancing himself on the floor with both arms, his right leg extended straight out and slightly tilted upward and his left leg pointed straight upward and slightly tilted backward\",\n",
    "    \"Gigachad doing a handstand wearing small black underwear and a tattoo on his lower abdominal muscles that says Berlin with the rest of the text cut off by the underwear\",\n",
    "    \"Gigachad oriented upside down in midair with his left arm extended toward the ground, his right arm extended to the left, and his legs extended upward in opposite directions\",\n",
    "    \"Gigachad pictured from the side doing a handstand with his chest puffed out and his legs extended perpendicularly to his body\",\n",
    "    \"Gigachad suspending himself off the ground with just his left arm\",\n",
    "    \"Gigachad looking down at the ground with his left bicep flexed and his right arm reaching behind his head, the text YES I HEARD ABOUT GIGA CHAD NFT COLLECTION SLEEK'N'TEARS by Krista Sudmalis on cryptorenaissance.org displayed\",\n",
    "    \"Gigachad pictured from the side with both hands behind his head and his right leg bent more than his left leg\",\n",
    "    \"Gigachad facing to the left with his pelvis thrusting forward and his hands on his hips wearing jeans and a black belt\",\n",
    "    \"Gigachad with his arms raised outward and facing up with an expression of seriousness and victory\"\n",
    "]\n",
    "dataset = ImageTextDataset(\n",
    "    image_dir=r\"C:\\Users\\Craig\\Desktop\\stable-diffusion-3-medium\\gigachads\",\n",
    "    prompts=prompts,\n",
    "    image_processor=pipeline.image_processor,\n",
    "    image_size=(1024, 1024)  # Set consistent size for all images\n",
    ")\n",
    "\n",
    "test_item = dataset[0]\n",
    "print(f\"Image type:\", type(test_item['image']))\n",
    "if isinstance(test_item['image'], dict):\n",
    "    print(\"Image keys:\", test_item['image'].keys())\n",
    "    if 'pixel_values' in test_item['image']:\n",
    "        print(\"Pixel values shape:\", test_item['image']['pixel_values'].shape)\n",
    "print(f\"Prompt: {test_item['prompt']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Fine-Tuning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_to_tensor = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def fine_tune_with_images(model, tokenizer, dataset, epochs=5, learning_rate=1e-4, batch_size=4):\n",
    "    accelerator = Accelerator()\n",
    "    optimizer = torch.optim.AdamW(model.text_encoder_2.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Set requires_grad for all parameters and ensure the model is in training mode\n",
    "    for param in model.text_encoder_2.parameters():\n",
    "        param.requires_grad = True\n",
    "    model.text_encoder_2.train()\n",
    "\n",
    "    # Prepare model and optimizer with accelerator\n",
    "    model.text_encoder_2, optimizer = accelerator.prepare(model.text_encoder_2, optimizer)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            prompts = batch[\"prompt\"]\n",
    "\n",
    "            # Generate images with the model without no_grad\n",
    "            outputs = model(\n",
    "                prompt=prompts,\n",
    "                num_inference_steps=28,\n",
    "                height=1024,\n",
    "                width=1024,\n",
    "                guidance_scale=7.0,\n",
    "            ).images\n",
    "\n",
    "            # Process target images\n",
    "            if isinstance(batch[\"image\"], dict) and \"pixel_values\" in batch[\"image\"]:\n",
    "                target_images = batch[\"image\"][\"pixel_values\"]\n",
    "            else:\n",
    "                target_images = batch[\"image\"]\n",
    "\n",
    "            # Ensure target images are on the correct device and dtype\n",
    "            target_images = target_images.to(device=accelerator.device, dtype=torch.float16)\n",
    "\n",
    "            # Convert each PIL image in `outputs` to a torch tensor\n",
    "            generated_images = torch.stack([\n",
    "                pil_to_tensor(image).to(device=accelerator.device, dtype=torch.float16) for image in outputs\n",
    "            ])\n",
    "\n",
    "            # Calculate loss, ensuring generated and target images have the same shape\n",
    "            if generated_images.shape != target_images.shape:\n",
    "                target_images = target_images.squeeze()\n",
    "            loss = nn.functional.mse_loss(generated_images, target_images)\n",
    "\n",
    "            # Ensure loss has requires_grad=True\n",
    "            loss.requires_grad = True\n",
    "\n",
    "            # Backpropagation\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Fine-Tuning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:18<00:00,  1.54it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfine_tune_with_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m, in \u001b[0;36mfine_tune_with_images\u001b[1;34m(model, tokenizer, dataset, epochs, learning_rate, batch_size)\u001b[0m\n\u001b[0;32m     21\u001b[0m prompts \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Generate images with the model without no_grad\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Process target images\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\diffusers\\pipelines\\stable_diffusion_3\\pipeline_stable_diffusion_3.py:935\u001b[0m, in \u001b[0;36mStableDiffusion3Pipeline.__call__\u001b[1;34m(self, prompt, prompt_2, prompt_3, height, width, num_inference_steps, timesteps, guidance_scale, negative_prompt, negative_prompt_2, negative_prompt_3, num_images_per_prompt, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, output_type, return_dict, joint_attention_kwargs, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, max_sequence_length)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     latents \u001b[38;5;241m=\u001b[39m (latents \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mscaling_factor) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mshift_factor\n\u001b[1;32m--> 935\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    936\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor\u001b[38;5;241m.\u001b[39mpostprocess(image, output_type\u001b[38;5;241m=\u001b[39moutput_type)\n\u001b[0;32m    938\u001b[0m \u001b[38;5;66;03m# Offload all models\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\diffusers\\utils\\accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\diffusers\\models\\autoencoders\\autoencoder_kl.py:326\u001b[0m, in \u001b[0;36mAutoencoderKL.decode\u001b[1;34m(self, z, return_dict, generator)\u001b[0m\n\u001b[0;32m    324\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(decoded_slices)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 326\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (decoded,)\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\diffusers\\models\\autoencoders\\autoencoder_kl.py:297\u001b[0m, in \u001b[0;36mAutoencoderKL._decode\u001b[1;34m(self, z, return_dict)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_quant_conv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    295\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_quant_conv(z)\n\u001b[1;32m--> 297\u001b[0m dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dec,)\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\diffusers\\models\\autoencoders\\vae.py:337\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, sample, latent_embeds)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;66;03m# up\u001b[39;00m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m up_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_blocks:\n\u001b[1;32m--> 337\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[43mup_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# post-process\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m latent_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_blocks.py:2750\u001b[0m, in \u001b[0;36mUpDecoderBlock2D.forward\u001b[1;34m(self, hidden_states, temb)\u001b[0m\n\u001b[0;32m   2748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2749\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers:\n\u001b[1;32m-> 2750\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mupsampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\diffusers\\models\\upsampling.py:169\u001b[0m, in \u001b[0;36mUpsample2D.forward\u001b[1;34m(self, hidden_states, output_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolate:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(hidden_states, size\u001b[38;5;241m=\u001b[39moutput_size, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Craig\\Desktop\\ChadGPT\\chagdpt\\Lib\\site-packages\\torch\\nn\\functional.py:4536\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   4534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mupsample_nearest1d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n\u001b[0;32m   4535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 4536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_nearest2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   4538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mupsample_nearest3d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fine_tune_with_images(pipeline, tokenizer, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [02:36<00:00,  5.60s/it]\n"
     ]
    }
   ],
   "source": [
    "gigachad_image = pipeline(\n",
    "    prompt=\"Gigachad\",\n",
    "    negative_prompt=\"\",\n",
    "    num_inference_steps=28,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=7.0,\n",
    ").images[0]\n",
    "\n",
    "gigachad_image.save(\"gigachad_inference_test_plain.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
